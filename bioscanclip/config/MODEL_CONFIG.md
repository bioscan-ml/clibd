## General Training Settings

- **batch_size: 500**
  The training batch size used during model training.

- **epochs: 30**
  The total number of training epochs.

- **wandb_project_name: CLIBD**
  The project name for Weights & Biases (wandb) tracking.

- **using_train_seen_for_pre_train: true**
  Whether to use the training seen data for pre-training.

- **dataset: bioscan_1m**
  The dataset used during training.

## Input Modalities

This section configures the input modalities and their associated models.

- **image**
  - **input_type: image**: Specifies that the input is an image.
  - **model: vit**: The model architecture used for processing images.

- **dna**
  - **input_type: sequence**: Specifies that the input is a sequence (DNA data).
  - **model: barcode_bert**: The model architecture used for processing DNA sequences.

- **language**
  - **input_type: sequence**: Specifies that the input is a text sequence.
  - **model: bert_small**: The model architecture used for processing language data.

## Model Output and Evaluation

- **model_output_name: image_dna_text_4gpu**
  The identifier for the model output, which may be used to name checkpoints and output directories.

- **evaluation_period: 1**
  The frequency (in epochs) at which evaluation is performed during training.

- **ckpt_path: ${project_root_path}/ckpt/bioscan_clip/final_experiments/image_dna_text_4gpu_50epoch/best.pth**
  The path to the checkpoint file for the pre-trained or best-performing model. Using to load checkpoint during inference.

- **output_dim: 768**
  The dimensionality of the output features generated by the model.

- **port: 29531**
  The port number for any distributed training or communication setup.

## Learning Rate and Optimization Settings

- **disable_lora: true**
  Whether to disable LoRA (Low-Rank Adaptation) modifications during training. Set to true to perform full fine-tuning.

- **lr_scheduler: one_cycle**
  The learning rate scheduler used (e.g., one-cycle policy).

- **lr_config**
  Learning rate configuration parameters:
  - **lr: 1e-6**: The starting learning rate.
  - **max_lr: 5e-5**: The maximum learning rate for the one-cycle policy.

- **all_gather: true**
  Whether to perform an all-gather operation during distributed training.

## Loss Setup and Additional Options

- **loss_setup**
  Settings for loss calculation and distributed gradient computation:
  - **gather_with_grad: true**: Gather operations that retain gradient information.
  - **use_horovod: false**: Indicates if Horovod is used for distributed training.
  - **local_loss: false**: Whether to compute loss locally on each node.

- **fix_temperature: false**
  Whether to fix the temperature parameter (if applicable) during training.

- **amp: true**
  Enable automatic mixed precision (AMP) for faster and memory-efficient training.

- **random_seed: false**
  Specifies if a random seed should be explicitly set (false indicates that it might use a default or externally set seed).

- **eval_skip_epoch: 23**
  Specifies the epoch from which evaluation might be skipped (if applicable).

- **default_seed: 42**
  The default random seed to ensure reproducibility.

## Fine-tuning Settings

- **fine_tuning_set**
  Fine-tuning configuration parameters:
  - **batch_size: 150**: The batch size used during fine-tuning.
  - **epochs: 15**: The number of epochs for fine-tuning.
  - **fine_tune_model_output_dir: ${model_output_dir}/${model_config.model_output_name}/supervise_fine_tune_ckpt**: The output directory for fine-tuned model checkpoints.

## Usage Instructions

1. **Environment Setup**
   Ensure that all dynamic paths (e.g., `${project_root_path}`) are correctly set in your environment, and that necessary files (datasets, checkpoints) are in place.

2. **Adjusting Parameters**
   Modify the training parameters (batch size, epochs, learning rate, etc.) as needed for your experimental setup.

3. **Training and Evaluation**
   Follow the provided scripts or instructions to start training, perform inference, or run evaluations. Monitor progress via wandb if enabled.

4. **Fine-tuning**
   When fine-tuning is required, refer to the fine-tuning settings section and adjust parameters accordingly.

## Notes

- Dynamic variables (e.g., `${project_root_path}`) are resolved at runtime. Verify that your environment variables and paths are properly configured.
- Some parameters might need further clarification or additional configuration based on your specific training needs.

